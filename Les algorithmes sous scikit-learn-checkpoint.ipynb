{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script:\n",
    "#   des algorithmes Random Forest (RF) Gradient Boosting (GBM),\n",
    "#   extrem Gradient Boosting (XGB) et Deep learning (DL) Sous Scikit-learn\n",
    "\n",
    "# Description:\n",
    "#Ce script est dédié au développement des algorithme RF, GBM, XGB et DL,\n",
    "#le développement de ces algorithme suit étapes suivant:\n",
    "#         1- Importation des bibliothéques et les données;\n",
    "#         2- Création des fonctions d'évaluation et d'affichage des variables Important;\n",
    "#         3- Développement des 3 algorithmes avec des paramètres Par défaut:\n",
    "#             3-1). RF avec son évaluation et l'affichage de ses variables important;\n",
    "#             3-2). GBM avec son évaluation et l'affichage de ses variables important;\n",
    "#             3-3). XGB avec son évaluation et l'affichage de ses variables important;\n",
    "#             3-4). DL avec son évaluation.\n",
    "#         4- Optimisation des hyperparamètres avec Random Search et Grid search;\n",
    "#         5- Répétition de l'étape 3 mais avec les paramètres séléctionné au l'étape 4.\n",
    "\n",
    "# Version:\n",
    "#     Mohammed AMEKSA:       Juin 2019       Script Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les bibliothèques et les données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les bibliothèques \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Importer le package de l'algorithme random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Importer le package de l'algorithme gradient boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Importer le package de l'algorithme xgboost\n",
    "import xgboost as XGBRegressor\n",
    "# # Importer le package de l'algorithme d'apprentissage profond\n",
    "from sklearn import neural_network\n",
    "# importer le package de standardisation pour deep learning\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "#Pour l'évaluation\n",
    "from sklearn import metrics\n",
    "#Optimisation des hyperparamètres du modèle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les deux fichiers de données\n",
    "dataset_train = pd.read_csv('Train-Equil-Lon-Lat-Hour-Month-RedVisi.csv')\n",
    "dataset_test = pd.read_csv('Test-Equil-Lon-Lat-Hour-Month-RedVisi.csv')\n",
    "\n",
    "#Détérminer pour chaque fichie les variables independants et le target\n",
    "#fichier d'entrainement\n",
    "X_train = dataset_train.iloc[:, 0:-1].values   \n",
    "y_train = dataset_train.iloc[:, -1].values     \n",
    "#fichier de test\n",
    "X_test = dataset_test.iloc[:, 0:-1].values     \n",
    "y_test = dataset_test.iloc[:, -1].values       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des fonctions d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle en calculant les 6 indices de pérformance\n",
    "# coefficient de correlation, biais, variance, MAE, MSE et RMSE\n",
    "def evaluate(model):\n",
    "    #prédire les données de test \n",
    "    y_pred = model.predict(X_test) \n",
    "    #évaluer le modèle\n",
    "    #Calculer le coefficient de correlation \n",
    "    cc=np.corrcoef(y_test, y_pred)[0, 1]\n",
    "    # Calculer le bias\n",
    "    bias=np.mean(y_pred)-np.mean(y_test)\n",
    "    # Calculer la variance \n",
    "    var = metrics.explained_variance_score(y_test,y_pred)\n",
    "    # Calculer le MAE (mean absolute error)\n",
    "    mea = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    # Calculer MSE (mean squared error)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    # Calculer RMSE (root mean squared error)\n",
    "    rmse = np.sqrt(mse)\n",
    "    #Afficher les résultats\n",
    "    print('Coefficient de Correlation: %.2f'%cc)\n",
    "    print('Biais: %.2f'%bias)\n",
    "    print('Variance: %.2f'%var)\n",
    "    print('MAE: %.5f' %mea)\n",
    "    print('MSE: %.5f' %mse)  \n",
    "    print('RMSE: %.5f' %rmse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traçer la courbe des variable importantes et afficher les 5 les plus importants\n",
    "def importante_features(model):\n",
    "    imp=model.feature_importances_\n",
    "    df_imp=pd.DataFrame(columns=['variables','% d\\'importance'])\n",
    "    d={}\n",
    "    for i in range(0, len(imp)):\n",
    "        df_imp=df_imp.append({'variables':dataset_train.columns[i], \n",
    "                              '% d\\'importance':imp[i]},ignore_index=True)\n",
    "    df_imp_sort=df_imp.sort_values(by='% d\\'importance', ascending=True,\n",
    "                                   na_position='first')\n",
    "    df_imp_sort.set_index('variables').plot( kind='barh',figsize=(10,10))\n",
    "    print(df_imp_sort.tail(5))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Méthodes Ensemblistes\n",
    "## Random Forest\n",
    "### Par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un instance de random forest\n",
    "rf_defaut = RandomForestRegressor(random_state=42) \n",
    "# Ajuster le modèle sur les données d'entrainement\n",
    "rf_defaut.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle Random Forest avec les paramètres par défauts\n",
    "evaluate(rf_defaut)\n",
    "# Afficher les variables importants du modèle Random Forest \n",
    "# avec les paramètres par défauts\n",
    "importante_features(rf_defaut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation des hyperparamètres avec Grid Search et Random Search\n",
    "**les hyperparamètres les plus importants choisis pour Random forest sont:**\n",
    "* n_estimators = nombre des arbres \n",
    "* max_features = nombre maximal d'entités prises en compte pour fractionner un nœud\n",
    "* max_depth = nombre maximum de niveaux dans chaque arbre de décision\n",
    "* min_samples_split = Nombre minimal d'échantillons requis pour scinder un nœud\n",
    "* min_samples_leaf = nombre min de points de données autorisés pour une feuille\n",
    "* bootstrap = méthode d'échantillonnage des points de données (avec ou sans remplacement) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "#  nombre des arbres pour random forest \n",
    "n_estimators_rf = [int(x) for x in np.linspace(start = 60, stop = 80, num = 5)]\n",
    "# Nombre d'observations à prendre en compte à chaque scission n/3 pour la régression\n",
    "features = dataset_train.columns[:-1]\n",
    "max_features_rf = ['auto', 'sqrt', 'log2', int(len(features)/3)]\n",
    "# Nombre maximum de niveaux dans une arbre \n",
    "# dans cette étude nous avons choisie une plage entre 1 et n/2 \n",
    "# avec n nombre d'observation\n",
    "max_depth_rf = [int(x) for x in np.linspace(1, 17, num = 4)]\n",
    "# Méthode d'échentillonage des points\n",
    "bootstrap_rf = [True, False]\n",
    "# Nombre minimal d'échantillons requis pour scinder un nœud\n",
    "min_samples_split_rf = list(range(2,5))\n",
    "# nombre min de points de données autorisés pour une feuille\n",
    "min_samples_leaf_rf= list(range(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de dictionnaire (random grid)\n",
    "rf_random_grid = {'n_estimators': n_estimators_rf,\n",
    "                    'max_features': max_features_rf,\n",
    "                    'max_depth': max_depth_rf,\n",
    "                    'bootstrap':bootstrap_rf,\n",
    "                    'min_samples_split': min_samples_split_rf,\n",
    "                    'min_samples_leaf': min_samples_leaf_rf,\n",
    "                 }\n",
    "# Utiliser ce dictionaire (random grid) pour chercher la combinaison\n",
    "# des hyperparamètres optimale\n",
    "rf_random_search = RandomizedSearchCV(estimator = RandomForestRegressor(),\n",
    "                               param_distributions = rf_random_grid,\n",
    "                               n_iter = 10, \n",
    "                               cv = 3, \n",
    "                               verbose=2, \n",
    "                               scoring='neg_mean_squared_error')\n",
    "# Ajuster le modèle sur les données d'entrainement\n",
    "rf_random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle Random Forest avec les paramètres choisis \n",
    "# par random search\n",
    "evaluate(rf_random_search)\n",
    "# Afficher les variables importants du modèle Random Forest \n",
    "# avec les paramètres choisis par random search\n",
    "importante_features(rf_random_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**\n",
    "* Vue que Grid Search ça prend du temps nous avons éssayer de faire l'optimisation comme suit:\n",
    "* premièrement nous avons optimiser (n_estimators) et (max_features)\n",
    "* utiliser les valeurs optimales de n_estimators et max_features pour optimiser min_samples_split et min_samples_leaf\n",
    "* Utiliser les quatres précidents pour optimiser le reste des paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiser n_estimators and max_features, puis utiliser les valeur optimales\n",
    "# choisis pour optimiser les autres\n",
    "param_grid_rf = [\n",
    "{'n_estimators': n_estimators_rf,\n",
    " 'max_features': max_features_rf,\n",
    "}\n",
    "]\n",
    "# Création du modèle\n",
    "grid_search_forest = GridSearchCV(RandomForestRegressor(),\n",
    "                                  param_grid_rf,                                  \n",
    "                                  cv=3, \n",
    "                                  scoring='neg_mean_squared_error',\n",
    "                     #pour que les résultats soit toujours les mêmes\n",
    "                                  random_state = 42,\n",
    "                                 )\n",
    "# Ajuster le modèle sur les données d'entrainement\n",
    "grid_search_forest.fit(X_train, y_train)\n",
    "# Afficher les mielleurs hyperparamètres choisis\n",
    "print(grid_search_forest.best_params_)\n",
    "# évaluation du modèle Random Forest avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_forest)\n",
    "# Afficher les variables importants du modèle Random Forest avec \n",
    "# les paramètres choisis par grid search\n",
    "importante_features(grid_search_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiser min_samples_split et min_samples_leaf, puis utiliser\n",
    "# les valeur optimales choisis pour optimiser les autres\n",
    "param_grid_rf = [\n",
    "{'min_samples_split': min_samples_split_rf,\n",
    " 'min_samples_leaf':min_samples_leaf_rf,\n",
    "}\n",
    "]\n",
    "# Création du modèle\n",
    "grid_search_forest = GridSearchCV(RandomForestRegressor(\n",
    "                        # ici nous spécifions les valeurs choisis\n",
    "                        #pour n_estimators et max_feautures\n",
    "                        # notre cas les valeurs suivants sont les optimales\n",
    "                                        n_estimators= 75 ,\n",
    "                                        max_features=11\n",
    "                                    ),\n",
    "                                  param_grid_rf,                                  \n",
    "                                  cv=3, \n",
    "                                  scoring='neg_mean_squared_error',\n",
    "                        #pour que les résultats soit toujours les mêmes\n",
    "                                  random_state = 42,\n",
    "                                 )\n",
    "# Ajuster le modèle sur les données d'entrainement\n",
    "grid_search_forest.fit(X_train, y_train)\n",
    "# Afficher les mielleurs hyperparamètres choisis\n",
    "print(grid_search_forest.best_params_)\n",
    "# évaluation du modèle Random Forest avec les paramètres \n",
    "# choisis par Grid search\n",
    "evaluate(grid_search_forest)\n",
    "# Afficher les variables importants du modèle Random Forest \n",
    "# avec les paramètres choisis par grid search\n",
    "importante_features(grid_search_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De même nous avons essayer d'optimiser max_depth et bootstrap, \n",
    "# puis utiliser les valeur optimales choisis pour optimiser les autres\n",
    "param_grid_rf = [\n",
    "{\n",
    " 'max_depth': max_depth_rf,\n",
    " 'bootstrap': bootstrap_rf,\n",
    "}\n",
    "]\n",
    "# Création du modèle\n",
    "grid_search_forest = GridSearchCV(RandomForestRegressor(\n",
    "                    # ici nous spécifions les valeurs choisis \n",
    "                    # pour n_estimators et max_feautures\n",
    "                    # notre cas les valeurs suivants sont les optimales\n",
    "                                        n_estimators= 75 ,\n",
    "                                        max_features=11,\n",
    "                                        min_samples_split=2,\n",
    "                                        min_samples_leaf=2,\n",
    "                                    ),\n",
    "                                  param_grid_rf,                                  \n",
    "                                  cv=3, \n",
    "                                  scoring='neg_mean_squared_error',\n",
    "                                  #pour que les résultats soit toujours les mêmes\n",
    "                                  random_state = 42,\n",
    "                                 )\n",
    "# Ajuster le modèle sur les données d'entrainement\n",
    "grid_search_forest.fit(X_train, y_train)\n",
    "# Afficher les mielleurs hyperparamètres choisis\n",
    "print(grid_search_forest.best_params_)\n",
    "# évaluation du modèle Random Forest avec les paramètres choisis \n",
    "# par Grid search\n",
    "evaluate(grid_search_forest)\n",
    "# Afficher les variables importants du modèle Random Forest avec \n",
    "# les paramètres choisis par grid search\n",
    "importante_features(grid_search_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine\n",
    "### Par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un instance de gradient boosting machine\n",
    "gbm_defaut = GradientBoostingRegressor(random_state=42)\n",
    "# Ajuster le modèle sur les données d'entrainement\n",
    "gbm_defaut.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle gradient boosting machine avec les paramètres \n",
    "# par défauts\n",
    "evaluate(gbm_defaut)\n",
    "# Afficher les variables importants du modèle gradient boosting machine \n",
    "# avec les paramètres par défauts\n",
    "importante_features(gbm_defaut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation des hyperparamètres avec Grid Search et Random Search\n",
    "**les hyperparamètres les plus importants choisis pour Gradient Boosting Machine sont:**\n",
    "\n",
    "Les paramètres généraux peuvent être divisés en 3 catégories:\n",
    "* Paramètres spécifiques aux arbres: ils affectent chaque arbre individuel du modèle.\n",
    "* Paramètres d'amplification: ils affectent l'opération d'amplification dans le modèle.\n",
    "* Paramètres divers: Autres paramètres pour le fonctionnement général.\n",
    "\n",
    "Les paramètres utilisés pour définir un arbre\n",
    "* min_samples_split: nombre minimal d'échantillons (ou d'observations) nécessaires dans un nœud à prendre en compte pour la scission.\n",
    "* min_samples_leaf: minimum d'échantillons (ou d'observations) requis dans un nœud terminal ou une feuille \n",
    "* max_depth: profondeur maximale d'un arbre.\n",
    "* max_feature: nombre d'observations à prendre en compte lors de la recherche du meilleur split. \n",
    "\n",
    "paramètres de gestion du boosting:\n",
    "* n_estimators: nombre d'arbres séquentiels à modéliser\n",
    "* learning_rate: détermine l'impact de chaque arbre sur le résultat final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les intérvale des paramètres\n",
    "# Nombre d'arbres\n",
    "n_estimators_gbm = [int(x) for x in np.linspace(start = 100, stop = 200, num = 5)]\n",
    "# nombre des observations à considirer pour un split \n",
    "#sqrt(n) pour classification et n/3 for régression\n",
    "features = dataset_train.columns[:-1] \n",
    "max_features_gbm = ['auto', 'sqrt', 'log2', int(len(features)/3)]\n",
    "# profondeur maximale d'un arbre\n",
    "max_depth_gbm = [int(x) for x in np.linspace(1, 15, num = 4)]\n",
    "# nombre minimal d'échantillons (ou d'observations) nécessaires \n",
    "# dans un nœud à prendre en compte pour la scission\n",
    "min_samples_split_gbm = list(range(2,5))\n",
    "# Taux d'apprentissage\n",
    "learning_rate_gbm = [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2]\n",
    "min_samples_leaf_gbm= list(range(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de dictionnaire (random grid)\n",
    "GBM_random_grid = {'max_features': max_features_gbm,\n",
    "                   'max_depth': max_depth_gbm,\n",
    "                   'min_samples_split': min_samples_split_gbm,\n",
    "                   'min_samples_leaf':min_samples_leaf_gbm,\n",
    "                   \n",
    "                   'n_estimators': n_estimators_gbm,\n",
    "                   'learning_rate': learning_rate_gbm,\n",
    "                  }\n",
    "\n",
    "# Utiliser ce dictionaire (random grid) pour chercher la combinaison \n",
    "# des hyperparamètres optimale\n",
    "random_search_GBM = RandomizedSearchCV(estimator = GradientBoostingRegressor(),\n",
    "                               param_distributions = GBM_random_grid,\n",
    "                               n_iter = 5, \n",
    "                               cv = 3, \n",
    "                               verbose=2, \n",
    "                                random_state = 42,\n",
    "                               scoring='neg_mean_squared_error')\n",
    "# Ajuster le modèle sur les données d'apprentissage\n",
    "random_search_GBM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle Gradient Boosting Machine avec les paramètres choisis \n",
    "# par random search\n",
    "evaluate(random_search_GBM)\n",
    "# Afficher les variables importants du modèle Gradient Boosting Machine avec\n",
    "# les paramètres choisis par random search\n",
    "importante_features(random_search_GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm = [\n",
    "    {'max_features': max_features_gbm,}\n",
    "]\n",
    "# Création du modèle\n",
    "grid_search_GBM = GridSearchCV(GradientBoostingRegressor(),\n",
    "                                param_grid_gbm, \n",
    "                                cv=3,\n",
    "                                random_state = 42,\n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_GBM.fit(X_train, y_train)\n",
    "# évaluation du modèle Gradient Boosting Machine avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_GBM)\n",
    "# Afficher les variables importants du modèle Gradient Boosting Machine avec\n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm = [\n",
    "    {'max_depth': max_depth_gbm,}\n",
    "]\n",
    "# Création du modèle avec les paramètres séléctionné précédement\n",
    "grid_search_GBM = GridSearchCV(GradientBoostingRegressor(max_features='auto'),\n",
    "                                param_grid_gbm, \n",
    "                                cv=3,\n",
    "                                random_state = 42,\n",
    "                                scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search_GBM.fit(X_train, y_train)\n",
    "\n",
    "# Ajustement du modèle\n",
    "grid_search_GBM.fit(X_train, y_train)\n",
    "# évaluation du modèle Gradient Boosting Machine avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_GBM)\n",
    "# Afficher les variables importants du modèle Gradient Boosting Machine avec \n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm = [\n",
    "    {'min_samples_split': min_samples_split_gbm,}\n",
    "]\n",
    "# Création du modèle avec les paramètres séléctionné précédement\n",
    "grid_search_GBM = GridSearchCV(GradientBoostingRegressor(max_features ='auto' , \n",
    "                                                         max_depth =10 ),\n",
    "                                param_grid_gbm, \n",
    "                                cv=3, \n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_GBM.fit(X_train, y_train)\n",
    "# évaluation du modèle Gradient Boosting Machine avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_GBM)\n",
    "# Afficher les variables importants du modèle Gradient Boosting Machine avec\n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm = [\n",
    "    {'min_samples_leaf':min_samples_leaf_gbm,}\n",
    "]\n",
    "# Création du modèle avec les paramètres séléctionné précédement\n",
    "grid_search_GBM = GridSearchCV(GradientBoostingRegressor(max_features= 'auto',\n",
    "                                                         max_depth= 10,\n",
    "                                                         min_samples_split = 3, ),\n",
    "                                param_grid_gbm, \n",
    "                                cv=3, \n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_GBM.fit(X_train, y_train)\n",
    "# évaluation du modèle Gradient Boosting Machine avec les paramètres choisis \n",
    "# par Grid search\n",
    "evaluate(grid_search_GBM)\n",
    "# Afficher les variables importants du modèle Gradient Boosting Machine avec \n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm = [\n",
    "    {'n_estimators': n_estimators_gbm }\n",
    "]\n",
    "# Création du modèle avec les paramètres séléctionné précédement\n",
    "grid_search_GBM = GridSearchCV(GradientBoostingRegressor(max_features='auto' ,\n",
    "                                                         max_depth = 10,\n",
    "                                                         min_samples_split =3 ,\n",
    "                                                         min_samples_leaf = 1 ,\n",
    "                                                        ),\n",
    "                                param_grid_gbm, \n",
    "                                cv=3, \n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_GBM.fit(X_train, y_train)\n",
    "# évaluation du modèle Gradient Boosting Machine avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_GBM)\n",
    "# Afficher les variables importants du modèle Gradient Boosting Machine avec \n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm = [\n",
    "    {'learning_rate': learning_rate_gbm,}\n",
    "]\n",
    "# Création du modèle avec les paramètres séléctionné précédement\n",
    "grid_search_GBM = GridSearchCV(GradientBoostingRegressor(max_features='auto',\n",
    "                                                         max_depth = 10,\n",
    "                                                         min_samples_split = 3,\n",
    "                                                         min_samples_leaf = 1,\n",
    "                                                         n_estimators = 200,\n",
    "                                                        ),\n",
    "                                param_grid_gbm, \n",
    "                                cv=3, \n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_GBM.fit(X_train, y_train)\n",
    "# évaluation du modèle Gradient Boosting Machine avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_GBM)\n",
    "# Afficher les variables importants du modèle Gradient Boosting Machine avec \n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eXtrem Gradient Boosting\n",
    "### Par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un instance d'eXtrem Gradient Boosting\n",
    "xgb_defaut = XGBRegressor(random_state=42)\n",
    "# Ajuster le modèle sur les données d'entrainement\n",
    "xgb_defaut.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle eXtrem Gradient Boosting avec les paramètres\n",
    "# par défauts\n",
    "evaluate(xgb_defaut)\n",
    "# Afficher les variables importants du modèle eXtrem Gradient Boosting \n",
    "# avec les paramètres par défauts\n",
    "importante_features(xgb_defaut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation des hyperparamètres avec Grid Search et Random Search\n",
    "**les hyperparamètres les plus importants choisis pour Gradient Boosting Machine sont:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les intérvalles de variations des hyperparamètres choisis pour XGB\n",
    "# Le taux d'apprentissage souvent entre 0.1 et 0.01 \n",
    "# or on peut essayer avec 0.15 0.2\n",
    "learning_rate_xgb = [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2]\n",
    "# nombre d'arbres, généralemnet 100 si la taille des données est elevé\n",
    "n_estimators_xgb =  [int(x) for x in np.linspace(start = 100, stop = 200, num = 11)]\n",
    "# profondeur des arbres utilisé dans le modèle\n",
    "max_depth_xgb = [int(x) for x in np.linspace(10, 15, num = 5)]\n",
    "# % des ligne séléctionés pour construire chaque arbre, \n",
    "# généralement prend des valeurs entre 0.8 et 1\n",
    "subsample_xgb = [i/100.0 for i in range(80,101,5)]\n",
    "# nombre de colonne utilisé par chaque arbre, \n",
    "# généralement prend des valeurs entre 0.3 et 0.8 \n",
    "# pour des problèmes où y a un grand nombre des colonnes\n",
    "# et entre 0.8 et 1 pour des problèmes avec un peu de colonnes \n",
    "colsample_bytree_xgb = [i/100.0 for i in range(80,101,5)]\n",
    "# il agit comme un paramètre de régularisation. 0, 1 ou 5\n",
    "gamma_xgb = [i for i in range(0,6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de dictionnaire (random grid)\n",
    "random_grid_XGB = {'max_depth': max_depth_xgb,\n",
    "                   'gamma': gamma_xgb,\n",
    "                   'colsample_bytree': colsample_bytree_xgb,\n",
    "                   'learning_rate': learning_rate_xgb,\n",
    "                   'n_estimators': n_estimators_xgb,\n",
    "                  }\n",
    "# Utiliser ce dictionaire (random grid) pour chercher la combinaison \n",
    "# des hyperparamètres optimales\n",
    "random_search_XGB = RandomizedSearchCV(estimator = XGBRegressor(),\n",
    "                               param_distributions = random_grid_XGB,\n",
    "                               n_iter = 10, \n",
    "                               cv = 3, \n",
    "                               verbose=2, \n",
    "                               random_state = 42,\n",
    "                               scoring='neg_mean_squared_error')\n",
    "# Ajuster le modèle sur les données d'apprentissage\n",
    "random_search_XGB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis \n",
    "# par random search\n",
    "evaluate(random_search_XGB)\n",
    "# Afficher les variables importants du modèle eXtrem Gradient Boosting\n",
    "# avec les paramètres choisis par random search\n",
    "importante_features(random_search_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionne n_estimators à optimiser en premier temps\n",
    "param_grid_xgb = [\n",
    "{'n_estimators': n_estimators_xgb,}\n",
    "]\n",
    "# Création du modèle \n",
    "grid_search_XGB = GridSearchCV(XGBRegressor(),\n",
    "                                param_grid_xgb, \n",
    "                                cv=3, \n",
    "                                random_state = 42,\n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "# évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_XGB)\n",
    "# Afficher les variables importants du modèle eXtrem Gradient Boosting avec\n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionne max_depth à optimiser \n",
    "param_grid_xgb = [\n",
    "{'max_depth': max_depth_xgb, }\n",
    "]\n",
    "# Création du modèle avec le nombre d'arbres séléctionné précédement\n",
    "# (dans notre cas =190)\n",
    "grid_search_XGB = GridSearchCV(XGBRegressor(n_estimators = 190),\n",
    "                                param_grid_xgb, \n",
    "                                cv=3, \n",
    "                               random_state = 42,\n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "# évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis \n",
    "# par Grid search\n",
    "evaluate(grid_search_XGB)\n",
    "# Afficher les variables importants du modèle eXtrem Gradient Boosting avec\n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionne subsample à optimiser \n",
    "param_grid_xgb = [\n",
    "{'subsample': subsample_xgb,}\n",
    "]\n",
    "# Création du modèle avec le nombre d'arbres séléctionné précédement \n",
    "#(dans notre cas n_estimators=190 et max_depth=11)\n",
    "grid_search_XGB = GridSearchCV(XGBRegressor(n_estimators =190 ,\n",
    "                                            max_depth = 11,\n",
    "                                           ),\n",
    "                                param_grid_xgb, \n",
    "                                cv=3, \n",
    "                                random_state = 42,\n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "# évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis \n",
    "# par Grid search\n",
    "evaluate(grid_search_XGB)\n",
    "# Afficher les variables importants du modèle eXtrem Gradient Boosting avec \n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner gamma à optimiser\n",
    "param_grid_xgb = [\n",
    "{'gamma': gamma_xgb,}\n",
    "]\n",
    "# Création du modèle avec le nombre d'arbres séléctionné précédement \n",
    "#(dans notre cas n_estimators=190 et max_depth=11 subsample est par défaut)\n",
    "grid_search_XGB = GridSearchCV(XGBRegressor(n_estimators = 190 ,\n",
    "                                            max_depth = 11,\n",
    "                                            ),\n",
    "                                param_grid_xgb, \n",
    "                                cv=3, \n",
    "                                random_state = 42,\n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "# évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_XGB)\n",
    "# Afficher les variables importants du modèle eXtrem Gradient Boosting avec \n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner colsample_bytree à optimiser\n",
    "param_grid_xgb = [\n",
    "{'colsample_bytree': colsample_bytree_xgb,}\n",
    "]\n",
    "# Création du modèle avec le nombre d'arbres séléctionné précédement \n",
    "#(dans notre cas n_estimators=190, max_depth=11, subsample est par défaut\n",
    "# et gamma=3)\n",
    "grid_search_XGB = GridSearchCV(XGBRegressor(n_estimators = 190,\n",
    "                                            max_depth = 11,\n",
    "                                            gamma = 3 ,\n",
    "                                            ),\n",
    "                                param_grid_xgb, \n",
    "                                cv=3,\n",
    "                                random_state = 42,\n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "# évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_XGB)\n",
    "# Afficher les variables importants du modèle eXtrem Gradient Boosting avec\n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner learning_rate à optimiser \n",
    "param_grid_xgb = [\n",
    "{'learning_rate': learning_rate_xgb,}\n",
    "]\n",
    "# Création du modèle avec le nombre d'arbres séléctionné précédement \n",
    "#(dans notre cas n_estimators=190, max_depth=11, subsample et \n",
    "# colsample_bytree sont par défaut et gamma=3)\n",
    "grid_search_XGB = GridSearchCV(XGBRegressor(n_estimators =190 ,\n",
    "                                            max_depth = 11,\n",
    "                                            gamma = 3 ,\n",
    "                                           ),\n",
    "                                param_grid_xgb,\n",
    "                                random_state = 42,\n",
    "                                cv=3, \n",
    "                                scoring='neg_mean_squared_error')\n",
    "# Ajustement du modèle\n",
    "grid_search_XGB.fit(X_train, y_train)\n",
    "# évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis\n",
    "# par Grid search\n",
    "evaluate(grid_search_XGB)\n",
    "# Afficher les variables importants du modèle eXtrem Gradient Boosting avec \n",
    "# les paramètres choisis par Grid search\n",
    "importante_features(grid_search_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Apprentissage profond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, nous avons standardisé les données en faisant appel \n",
    "# à un instance de StandardScaler\n",
    "# cette instance suit la régle \"x-men/std\"\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un instance de MLPRegressor avec les paramètres par défauts\n",
    "MLP_defaut=neural_network.MLPRegressor(random_state = 42)\n",
    "# Ajuster le modèle sur les données d'entrainement\n",
    "MLP_defaut.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle MLP avec les paramètres choisis par défauts\n",
    "evaluate(MLP_defaut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensuite, nous créons une instance du modèle Multilayer perceptrons (réseau de neurons), nous définirons hidden_layer_sizes. Pour ce paramètre, nous transmettrons une paire composée du nombre de neurones que nous voulons au niveau de chaque couche, la nième entrée du tuple représente le nombre de neurones de la nième couche du modèle MLP. \n",
    "* pour la simplicité, nous choisirons 2 couches avec le même nombre de neurones (68 nombre d'entrées fois 2) ainsi que 100 itérations maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_manuel=neural_network.MLPRegressor(hidden_layer_sizes=(68,68),\n",
    "                                       max_iter=100,\n",
    "                                       activation='relu',\n",
    "                                       random_state = 42,\n",
    "                                      )\n",
    "MLP_manuel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# évaluation du modèle MLP avec les paramètres choisis manuellement\n",
    "evaluate(MLP_manuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
